{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e503afc",
   "metadata": {},
   "source": [
    "# 1. explain note book"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f5f65d",
   "metadata": {},
   "source": [
    "all data set  indexed by id_patiant and time per hour:\n",
    "\n",
    "    1- all_df.pickle: X_data whithout processing\n",
    "    \n",
    "    2- all_original labels.pickle: Y_data also whithout preprocessing\n",
    "\n",
    "the beginer observations from each file as past time (80%)represent the train set data \n",
    "\n",
    "and the last observations  as future time represent the test sey data (20%)\n",
    "\n",
    "1) train set data indexed by id_patiant and per hour time: \n",
    "\n",
    "    1-1 train_set.pickle: X_train whithout processing\n",
    "    \n",
    "    1-2 train_original labels.pickle: Y_train also whithout preprocessing\n",
    "    \n",
    "2) test set data indexed by id_patiant and per hour time:\n",
    "\n",
    "    2-1- test_set.pickle: X_test whithout processing\n",
    "    2-2- test_original labels.pickle: Y_test also whithout preprocessing\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "087f7cd2",
   "metadata": {},
   "source": [
    "# 2. libreries  import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "id": "ac5c249a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np, os, os.path, sys\n",
    "import dill, pickle\n",
    "from multiprocessing import Pool, cpu_count\n",
    "from joblib import Parallel, delayed\n",
    "import json\n",
    "from sty import fg, bg\n",
    "import pandas as pd\n",
    "from math import floor\n",
    "import shutil\n",
    "import fidle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "677dad88",
   "metadata": {},
   "source": [
    "# 3. parametres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "id": "0de0f049",
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_DIR = os.path.dirname(os.path.abspath('vedio RNN'))\n",
    "\n",
    "#----------------------------------------------------inputs---------------------------------\n",
    "# Setup paths\n",
    "\n",
    "DATA_DIR = ROOT_DIR + '/data/test/1000AB'\n",
    "input_directory = 'C:/Users/hadjali/1 dataset/training_setAB_1000' #500/500 patient from  hospitals A/B\n",
    "\n",
    "#DATA_DIR = ROOT_DIR + '/data/test/4AB'\n",
    "#input_directory = 'C:/Users/hadjali/1 dataset/training_set_AB_4' #2/2 patient from  hospitals A/B\n",
    "\n",
    "#DATA_DIR = ROOT_DIR + '/data/test/500A'\n",
    "#input_directory='C:/Users/hadjali/1 dataset/training_set_A_500' #500 from hospital A\n",
    "\n",
    "#DATA_DIR = ROOT_DIR + '/data/test/500B'\n",
    "#input_directory='C:/Users/hadjali/1 dataset/training_set_B_500' #500 from hospital B\n",
    "\n",
    "#DATA_DIR = ROOT_DIR + '/data/test/40336AB'\n",
    "#input_directory='C:/Users/hadjali/1 dataset/training_set_AB_40336' #40336 from hospital B and B\n",
    "\n",
    "#DATA_DIR = ROOT_DIR + '/data/test/20AB'\n",
    "#input_directory='C:/Users/hadjali/1 dataset/training_set_AB_20' #20 from hospital B and B\n",
    "\n",
    "#DATA_DIR = ROOT_DIR + '/data/test/20336A'\n",
    "#input_directory = 'C:/Users/hadjali/1 dataset/training_set_A_20336' # 20336 patient from  hospitals A\n",
    "\n",
    "#DATA_DIR = ROOT_DIR + '/data/test/20000B'\n",
    "#input_directory = 'C:/Users/hadjali/1 dataset/training_set_B_20000' # 20000 patient from  hospitals B\n",
    "\n",
    "#DATA_DIR = ROOT_DIR + '/data/test/p100005 B'\n",
    "#input_directory='C:/Users/hadjali/1 dataset/p100005 B' # a nonseptic patient from hospital B\n",
    "\n",
    "#DATA_DIR = ROOT_DIR + '/data/test/p000009 A'\n",
    "#input_directory='C:/Users/hadjali/1 dataset/p000009 A' #a septic patient from hospital A\n",
    "\n",
    "scale       =1 #pourcentage of dataset to be used (1=all dataset)\n",
    "\n",
    "# if using future/past split method\n",
    "#train_prop  =.8 #poucentage for train (the rest being for the test)\n",
    "\n",
    "# if using paire/impaire split method\n",
    "train_prop  =80 #poucentage for train (the rest being for the test)\n",
    "val_prop    =70 #poucentage for subset train (the rest being for the validation set)\n",
    "\n",
    "#---------------------------------results--------------------------------------------\n",
    "#DATA_DIR + '/mean'\n",
    "#for data,label,df_value in [('all_df',labels,df_values), ('train_set',labels_train,train_values), ('test_set',labels_test,test_values)]:\n",
    "    #save_pickle(label, DATA_DIR + '/from_raw/Y/'+ data + '_original_labels.pickle')\n",
    "    #save_pickle(df_value, DATA_DIR + '/from_raw/X/'+ data + '_raw.pickle')\n",
    "    #x_y=pd.concat([df_values,labels],axis=1)\n",
    "    #save_pickle(x_y, DATA_DIR + '/from_raw/XY/'+ data + '_x_y_raw.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aedf799",
   "metadata": {},
   "source": [
    "# 4. functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "id": "b3cb9adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_pickle(obj, filename, use_dill=False, protocol=4):\n",
    "    \"\"\" Basic pickle/dill dumping \"\"\"\n",
    "    create_folder_if_not_exist(filename)\n",
    "    with open(filename, 'wb') as file:\n",
    "        if not use_dill:\n",
    "            pickle.dump(obj, file, protocol=protocol)\n",
    "        else:\n",
    "            dill.dump(obj, file)\n",
    "\n",
    "\n",
    "def load_pickle(filename, use_dill=False):\n",
    "    \"\"\" Basic pickle/dill loading function \"\"\"\n",
    "    with open(filename, 'rb') as file:\n",
    "        if not use_dill:\n",
    "            obj = pickle.load(file)\n",
    "        else:\n",
    "            obj = dill.load(file)\n",
    "    return obj\n",
    "\n",
    "\n",
    "def create_folder_if_not_exist(filename):\n",
    "    #Makes a folder if the path does not already exist\n",
    "    os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
    "\n",
    "def psv_to_dataframe(fname,scale=1):\n",
    "    \"\"\" Transforms a single psv file into a dataframe with an id, ICULOS columns\"\"\"\n",
    "    df = pd.read_csv(fname, sep='|')\n",
    "    df['id'] = (fname.split('.psv')[0].split('/')[-1])\n",
    "    df['patient'] = (fname.split('.psv')[0].split('/')[-1])\n",
    "    df['patient1'] = (fname.split('.psv')[0].split('/')[-1])\n",
    "    df['time'] = df['ICULOS']\n",
    "    # Idx according to id and time\n",
    "    df.set_index(['id','time'],inplace=True)\n",
    "    df=df[:int(scale*len(df))]\n",
    "    return df\n",
    "\n",
    "\n",
    "def validation_test(train,val_prop=70):\n",
    "    \n",
    "    #train=load_pickle( DATA_DIR + '/from_raw/XY/train_set_x_y_raw.pickle')\n",
    "    all_train_set=train\n",
    "    train=train[:]\n",
    "    patients_training_data = all_train_set['patient'].unique()\n",
    "    #print(len(patients_training_data))\n",
    "    #np.random.shuffle(patients_training_data) # melanger les patients\n",
    "    from math import floor\n",
    "    i=floor((val_prop*patients_training_data.shape[0])//100) # chois d'un entier  comme 80% de train\n",
    "    #print(i)\n",
    "    patients_training_data = patients_training_data[0:i]\n",
    "    sub_train = train[train['patient'].isin(patients_training_data)]#.drop('patient', axis=1)\n",
    "    val = train[~train['patient'].isin(patients_training_data)]#.drop('patient', axis=1)\n",
    "    #subset_train=train\n",
    "    return sub_train,val\n",
    "        \n",
    "def raw_data_transformation_to_raw_values_and_labels(location):\n",
    "    \n",
    "    \"\"\"\n",
    "    Given either a location of psv files or single psv file, transforms into dataframes indexed with id and time ICULOS\n",
    "\n",
    "    :param location: either a folder containing psv files or a single psv file\n",
    "    :return: df or (df,dataset_train,dataset_test) of all psv files indexed by [id, ICULOS]\n",
    "    \"\"\"\n",
    "\n",
    "    if isinstance(location, list):\n",
    "        fnames = [l + '/' + x for l in location for x in os.listdir(l)]\n",
    "    elif not location.endswith('.psv'):\n",
    "        fnames = [location + '/' + x for x in os.listdir(location)]\n",
    "    else:\n",
    "        fnames = [location]\n",
    "    #----------------------------------\n",
    "    is_septic_list=[]\n",
    "    non_septic_list=[]\n",
    "    for fname in fnames:\n",
    "        df= psv_to_dataframe(fname)\n",
    "        if np.any(df['SepsisLabel']):\n",
    "            is_septic_list.append(df)\n",
    "        else:\n",
    "            non_septic_list.append(df)\n",
    "            \n",
    "    \n",
    "    #----------------------------------\n",
    "\n",
    "    if(len(fnames)==1):\n",
    "        df= psv_to_dataframe(fnames[0])\n",
    "        sub_train_set,val_set=train_set=test_set=df\n",
    "    else:\n",
    "        i=floor((train_prop*len(fnames))/100) # chois d'un entier  comme 80% de train\n",
    "        #print(i)\n",
    "        j=0\n",
    "\n",
    "        list_df,list_train_set,list_test_set=[],[],[]\n",
    "        for fname in fnames:\n",
    "            df= psv_to_dataframe(fname)\n",
    "            list_df.append(df)\n",
    "            #if np.any(df['SepsisLabel']):\n",
    "                #is_septic=True\n",
    "\n",
    "            if (j<i):\n",
    "                list_train_set.append(df)\n",
    "            else:\n",
    "                list_test_set.append(df)\n",
    "            j=j+1\n",
    "            \"\"\"list_train_set.append(train_set)\n",
    "            list_test_set.append(test_set)\"\"\"\n",
    "\n",
    "        df = pd.concat(list_df)\n",
    "        train_set = pd.concat(list_train_set)\n",
    "        test_set = pd.concat(list_test_set)\n",
    "        if(len(train_set['patient'].unique())==1):\n",
    "            sub_train_set=val_set=train_set\n",
    "        else:\n",
    "            sub_train_set,val_set=validation_test(train_set,val_prop=70)\n",
    "        \n",
    "    # Get values and labels\n",
    "    if 'SepsisLabel' in df.columns:\n",
    "        df_values, labels = df.drop('SepsisLabel', axis=1), df['SepsisLabel']\n",
    "    else:\n",
    "        df_values = df\n",
    "\n",
    "    if 'SepsisLabel' in train_set.columns:\n",
    "        train_values, labels_train = train_set.drop('SepsisLabel', axis=1), train_set['SepsisLabel']\n",
    "    else:\n",
    "        train_values = train_set\n",
    "\n",
    "    if 'SepsisLabel' in test_set.columns:\n",
    "        test_values, labels_test = test_set.drop('SepsisLabel', axis=1), test_set['SepsisLabel']\n",
    "    else:\n",
    "        test_values = test_set\n",
    "    \n",
    "    if 'SepsisLabel' in sub_train_set.columns:\n",
    "        sub_train_values, sub_labels_train = sub_train_set.drop('SepsisLabel', axis=1), sub_train_set['SepsisLabel']\n",
    "    else:\n",
    "        sub_train_values = sub_train_set\n",
    "    \n",
    "    if 'SepsisLabel' in val_set.columns:\n",
    "        val_values, labels_val = val_set.drop('SepsisLabel', axis=1), val_set['SepsisLabel']\n",
    "    else:\n",
    "        val_values = val_set\n",
    "        \n",
    "    #---------------------calcule mean std from train set to be used for standrdiztion of data\n",
    "    df_mean_std = train_values.describe().loc[['mean', 'std']]\n",
    "    \n",
    "    #save_pickle(df_mean_std, DATA_DIR + '/mean/mean_std_scaling_'+f'{train_values.shape[0]}'+'.pickle')\n",
    "    save_pickle(df_mean_std, DATA_DIR + '/mean/mean_std.pickle')\n",
    "    \n",
    "    #----------------------------------------------------------------------------------------\n",
    "    for data,label,df_value in [('all_df',labels,df_values), ('train_set',labels_train,train_values),\n",
    "                                ('test_set',labels_test,test_values), ('sub_train_set',sub_labels_train,sub_train_values),\n",
    "                               ('val_set',labels_val,val_values)]:\n",
    "        \n",
    "        save_pickle(label, DATA_DIR + '/from_raw/Y/'+ data + '_original_labels.pickle')\n",
    "        save_pickle(df_value, DATA_DIR + '/from_raw/X/'+ data + '_raw.pickle')\n",
    "        \n",
    "        x_y=pd.concat([df_value,label],axis=1)\n",
    "        save_pickle(x_y, DATA_DIR + '/from_raw/XY/'+ data + '_x_y_raw.pickle')\n",
    "    #return df, df_values, labels,  train_set,train_values, labels_train,  test_set,test_values, labels_test\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "id": "f4f06492",
   "metadata": {},
   "outputs": [],
   "source": [
    "def raw_data_transformation_to_raw_values_and_labels_is_is_not_septic(is_is_not_list):\n",
    "\n",
    "    if(len(is_is_not_list)==1):\n",
    "        sub_train_set=val_set=train_set=test_set=is_is_not_list[0]\n",
    "    else:\n",
    "        #if(is_is_not_list=True)\n",
    "        i=floor((80*len(is_is_not_list))//100) # chois d'un entier  comme 80% de train\n",
    "        #print(i)\n",
    "        j=0\n",
    "\n",
    "        list_train_set,list_test_set=[],[]\n",
    "        for is_is_not in is_is_not_list:\n",
    "            #list_df.append(df)\n",
    "            if (j<i):\n",
    "                list_train_set.append(is_is_not)\n",
    "            else:\n",
    "                list_test_set.append(is_is_not)\n",
    "            j=j+1\n",
    "            \"\"\"list_train_set.append(train_set)\n",
    "            list_test_set.append(test_set)\"\"\"\n",
    "\n",
    "        #df = pd.concat(list_df)\n",
    "        train_set = pd.concat(list_train_set)\n",
    "        test_set = pd.concat(list_test_set)\n",
    "\n",
    "        \"\"\"if(len(train_set['patient'].unique())==1):\n",
    "            sub_train_set=val_set=train_set\n",
    "        else:\"\"\"\n",
    "    sub_train_set,val_set=validation_test(train_set,val_prop=70)\n",
    "        \n",
    "        \n",
    "\n",
    "    return train_set, test_set, sub_train_set,val_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "id": "6d3f8c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def raw_data_transformation_to_raw_values_and_labels1(location):\n",
    "    \n",
    "    \"\"\"\n",
    "    Given either a location of psv files or single psv file, transforms into dataframes indexed with id and time ICULOS\n",
    "\n",
    "    :param location: either a folder containing psv files or a single psv file\n",
    "    :return: df or (df,dataset_train,dataset_test) of all psv files indexed by [id, ICULOS]\n",
    "    \"\"\"\n",
    "\n",
    "    if isinstance(location, list):\n",
    "        fnames = [l + '/' + x for l in location for x in os.listdir(l)]\n",
    "    elif not location.endswith('.psv'):\n",
    "        fnames = [location + '/' + x for x in os.listdir(location)]\n",
    "    else:\n",
    "        fnames = [location]\n",
    "    #----------------------------------\n",
    "    list_df=[]\n",
    "    \n",
    "    if(len(fnames)==1):\n",
    "        df1= psv_to_dataframe(fnames[0])\n",
    "        list_df.append(df1)\n",
    "        df = pd.concat(list_df)\n",
    "        sub_train_set=val_set=train_set=test_set=df\n",
    "    else:\n",
    "        \n",
    "        \n",
    "        is_septic_list=[]\n",
    "        non_septic_list=[]\n",
    "        for fname in fnames:\n",
    "            df1= psv_to_dataframe(fname)\n",
    "            list_df.append(df1)\n",
    "            if np.any(df1['SepsisLabel']):\n",
    "                is_septic_list.append(df1)\n",
    "            else:\n",
    "                non_septic_list.append(df1)\n",
    "\n",
    "        print ('number of positif patient  ',len(is_septic_list))\n",
    "        print ('number of negatif patient  ',len(non_septic_list))\n",
    "        #df = pd.concat(list_df)\n",
    "        is_train_set, is_test_set, is_sub_train_set,is_val_set=raw_data_transformation_to_raw_values_and_labels_is_is_not_septic(is_septic_list)        \n",
    "        is_not_train_set, is_not_test_set, is_not_sub_train_set,is_not_val_set=raw_data_transformation_to_raw_values_and_labels_is_is_not_septic(non_septic_list)\n",
    "        #----------------------------------\n",
    "\n",
    "        df = pd.concat(list(list_df))\n",
    "        #print(df.shape)\n",
    "\n",
    "        #train_set = pd.concat(is_train_set)\n",
    "        #no_train_set = pd.concat(is_not_train_set)\n",
    "        train_set = pd.concat([is_train_set,is_not_train_set], axis=0)\n",
    "\n",
    "        test_set = pd.concat([is_test_set,is_not_test_set],axis=0)\n",
    "        #test_set = pd.concat(is_not_test_set)\n",
    "\n",
    "        sub_train_set = pd.concat([is_sub_train_set, is_not_sub_train_set],axis=0)\n",
    "        #sub_train_set = pd.concat(is_not_sub_train_set)\n",
    "\n",
    "        val_set = pd.concat([is_val_set, is_not_val_set], axis=0)\n",
    "        #val_set = pd.concat(is_not_val_set)\n",
    "\n",
    "    # Get values and labels\n",
    "    if 'SepsisLabel' in df.columns:\n",
    "        df_values, labels = df.drop('SepsisLabel', axis=1), df['SepsisLabel']\n",
    "    else:\n",
    "        df_values = df\n",
    "\n",
    "    if 'SepsisLabel' in train_set.columns:\n",
    "        train_values, labels_train = train_set.drop('SepsisLabel', axis=1), train_set['SepsisLabel']\n",
    "    else:\n",
    "        train_values = train_set\n",
    "\n",
    "    if 'SepsisLabel' in test_set.columns:\n",
    "        test_values, labels_test = test_set.drop('SepsisLabel', axis=1), test_set['SepsisLabel']\n",
    "    else:\n",
    "        test_values = test_set\n",
    "    \n",
    "    if 'SepsisLabel' in sub_train_set.columns:\n",
    "        sub_train_values, sub_labels_train = sub_train_set.drop('SepsisLabel', axis=1), sub_train_set['SepsisLabel']\n",
    "    else:\n",
    "        sub_train_values = sub_train_set\n",
    "    \n",
    "    if 'SepsisLabel' in val_set.columns:\n",
    "        val_values, labels_val = val_set.drop('SepsisLabel', axis=1), val_set['SepsisLabel']\n",
    "    else:\n",
    "        val_values = val_set\n",
    "        \n",
    "    #---------------------calcule mean std from train set to be used for standrdiztion of data\n",
    "    df_mean_std = train_values.describe().loc[['mean', 'std']]\n",
    "    \n",
    "    #save_pickle(df_mean_std, DATA_DIR + '/mean/mean_std_scaling_'+f'{train_values.shape[0]}'+'.pickle')\n",
    "    save_pickle(df_mean_std, DATA_DIR + '/mean/mean_std.pickle')\n",
    "    \n",
    "    #----------------------------------------------------------------------------------------\n",
    "    for data,label,df_value in [('all_df',labels,df_values), ('train_set',labels_train,train_values),\n",
    "                                ('test_set',labels_test,test_values), ('sub_train_set',sub_labels_train,sub_train_values),\n",
    "                               ('val_set',labels_val,val_values)]:\n",
    "        \n",
    "        save_pickle(label, DATA_DIR + '/from_raw/Y/'+ data + '_original_labels.pickle')\n",
    "        save_pickle(df_value, DATA_DIR + '/from_raw/X/'+ data + '_raw.pickle')\n",
    "        \n",
    "        x_y=pd.concat([df_value,label],axis=1)\n",
    "        save_pickle(x_y, DATA_DIR + '/from_raw/XY/'+ data + '_x_y_raw.pickle')\n",
    "    #return df, df_values, labels,  train_set,train_values, labels_train,  test_set,test_values, labels_test\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "844ce0b8",
   "metadata": {},
   "source": [
    "# 5. main call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "id": "538f9d91",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of positif patient   71\n",
      "number of negatif patient   929\n",
      "56\n",
      "56\n",
      "39\n",
      "743\n",
      "743\n",
      "520\n",
      "(38890, 43)\n"
     ]
    }
   ],
   "source": [
    "#df,df_values,labels,  train_set,train_values,labels_train,  test_set,test_values,labels_test =raw_data_transformation_to_raw_values_and_labels(input_directory)\n",
    "raw_data_transformation_to_raw_values_and_labels1(input_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e0c94a6",
   "metadata": {},
   "source": [
    "# 6. few statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "id": "e0d1402d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<br>**-------------all_df   ----------------**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pourcentage **** all df*************    100  %\n",
      "pourcentage **** all patient*************    100  %\n",
      "\n",
      "\n",
      "pourcentage **** train_set*************    79.28259192594497\n",
      "pourcentage *** test_set*************    20.717408074055026\n",
      "\n",
      "\n",
      "pourcentage **** train patients *************    79.9\n",
      "pourcentage **** test patients *************    20.1\n",
      "--------------\n",
      "number of all patients  all_df      1000\n",
      "number of positif patient   71\n",
      "number of negatif patient   929\n",
      "\n",
      "\n",
      "number of all observations for positif patients    4703\n",
      "where :::::::::  all_df  \n",
      " 0    4029\n",
      "1     674\n",
      "Name: SepsisLabel, dtype: int64\n",
      "\n",
      "\n",
      "number of all observations for negatif paients    34187\n",
      "--------------\n",
      "all observations  all_df      38890\n",
      "where :::  all_df  \n",
      " 0    38216\n",
      "1      674\n",
      "Name: SepsisLabel, dtype: int64\n",
      "--------------\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "<br>**-------------train_set   ----------------**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pourcentage *** sub_train_set*************    69.15966659099017\n",
      "pourcentage *** val_set *************    30.840333409009826\n",
      "\n",
      "\n",
      "pourcentage **** sub_train patients *************    69.96245306633291\n",
      "pourcentage **** val patients *************    30.037546933667084\n",
      "--------------\n",
      "number of all patients  train_set      799\n",
      "number of positif patient   56\n",
      "number of negatif patient   743\n",
      "\n",
      "\n",
      "number of all observations for positif patients    3456\n",
      "where :::::::::  train_set  \n",
      " 0    2923\n",
      "1     533\n",
      "Name: SepsisLabel, dtype: int64\n",
      "\n",
      "\n",
      "number of all observations for negatif paients    27377\n",
      "--------------\n",
      "all observations  train_set      30833\n",
      "where :::  train_set  \n",
      " 0    30300\n",
      "1      533\n",
      "Name: SepsisLabel, dtype: int64\n",
      "--------------\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "<br>**-------------test_set   ----------------**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------\n",
      "number of all patients  test_set      201\n",
      "number of positif patient   15\n",
      "number of negatif patient   186\n",
      "\n",
      "\n",
      "number of all observations for positif patients    1247\n",
      "where :::::::::  test_set  \n",
      " 0    1106\n",
      "1     141\n",
      "Name: SepsisLabel, dtype: int64\n",
      "\n",
      "\n",
      "number of all observations for negatif paients    6810\n",
      "--------------\n",
      "all observations  test_set      8057\n",
      "where :::  test_set  \n",
      " 0    7916\n",
      "1     141\n",
      "Name: SepsisLabel, dtype: int64\n",
      "--------------\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "<br>**-------------sub_train_set   ----------------**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------\n",
      "number of all patients  sub_train_set      559\n",
      "number of positif patient   39\n",
      "number of negatif patient   520\n",
      "\n",
      "\n",
      "number of all observations for positif patients    2284\n",
      "where :::::::::  sub_train_set  \n",
      " 0    1909\n",
      "1     375\n",
      "Name: SepsisLabel, dtype: int64\n",
      "\n",
      "\n",
      "number of all observations for negatif paients    19040\n",
      "--------------\n",
      "all observations  sub_train_set      21324\n",
      "where :::  sub_train_set  \n",
      " 0    20949\n",
      "1      375\n",
      "Name: SepsisLabel, dtype: int64\n",
      "--------------\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "<br>**-------------val_set   ----------------**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------\n",
      "number of all patients  val_set      240\n",
      "number of positif patient   17\n",
      "number of negatif patient   223\n",
      "\n",
      "\n",
      "number of all observations for positif patients    1172\n",
      "where :::::::::  val_set  \n",
      " 0    1014\n",
      "1     158\n",
      "Name: SepsisLabel, dtype: int64\n",
      "\n",
      "\n",
      "number of all observations for negatif paients    8337\n",
      "--------------\n",
      "all observations  val_set      9509\n",
      "where :::  val_set  \n",
      " 0    9351\n",
      "1     158\n",
      "Name: SepsisLabel, dtype: int64\n",
      "--------------\n"
     ]
    }
   ],
   "source": [
    "for data,label,df_value in [('all_df','all labels','all df_values'), ('train_set','labels_train','train_values'), \n",
    "                            ('test_set','labels_test','test_values'),\n",
    "                           ('sub_train_set','sub_labels_train','sub_train_values'), ('val_set','labels_val','val_values') ]:\n",
    "    fidle.utils.subtitle('-------------' + f'{data}'+'   ----------------')\n",
    "    \n",
    "    #labels=load_pickle( DATA_DIR + '/from_raw/Y/'+ data + '_original_labels.pickle')\n",
    "    #print(f'{label}'+ '  shape y ',labels.shape)\n",
    "    \n",
    "    #x_values=load_pickle( DATA_DIR + '/from_raw/X/'+ data + '_raw.pickle')\n",
    "    #print(f'{df_value}'+ ' shape x ',x_values.shape)\n",
    "    #display(f'{df_value}'+' display  x ',x_values.head())\n",
    "        \n",
    "    x_y_values=load_pickle( DATA_DIR + '/from_raw/XY/'+ data + '_x_y_raw.pickle')\n",
    "    \n",
    "    if(data=='all_df'):\n",
    "        all_size=x_y_values.shape[0]\n",
    "        all_patient=len(x_y_values['patient'].unique())\n",
    "        print('pourcentage **** all df*************   ', '100' +'  %')\n",
    "        print('pourcentage **** all patient*************   ', '100' +'  %')\n",
    "        print(\"\\n\")\n",
    "        train_x_y_values=load_pickle( DATA_DIR + '/from_raw/XY/train_set_x_y_raw.pickle')\n",
    "        #else:\n",
    "        #if(data=='train_set'):\n",
    "        train_size=train_x_y_values.shape[0]\n",
    "        train_patient=len(train_x_y_values['patient'].unique())\n",
    "        print('pourcentage **** train_set*************   ',train_x_y_values.shape[0]* 100/all_size )\n",
    "        #print('pourcentage **** train patients *************   ',train_patient* 100/all_patient )\n",
    "        \n",
    "        test_x_y_values=load_pickle( DATA_DIR + '/from_raw/XY/test_set_x_y_raw.pickle')\n",
    "        #else:\n",
    "        #if(data=='test_set'):\n",
    "        test_size=test_x_y_values.shape[0]\n",
    "        test_patient=len(test_x_y_values['patient'].unique())\n",
    "        print('pourcentage *** test_set*************   ',test_x_y_values.shape[0]* 100/all_size )\n",
    "        print(\"\\n\")\n",
    "        print('pourcentage **** train patients *************   ',train_patient* 100/all_patient )\n",
    "        print('pourcentage **** test patients *************   ',test_patient* 100/all_patient )\n",
    "        \n",
    "        #else:\n",
    "    if(data=='train_set'):\n",
    "\n",
    "        sub_train_x_y_values=load_pickle( DATA_DIR + '/from_raw/XY/sub_train_set_x_y_raw.pickle')\n",
    "        #if(data=='train_set'):\n",
    "        train_size=x_y_values.shape[0]\n",
    "        train_patient=len(x_y_values['patient'].unique())\n",
    "\n",
    "        sub_train_set_size=sub_train_x_y_values.shape[0]\n",
    "        sub_train_patient=len(sub_train_x_y_values['patient'].unique())\n",
    "        print('pourcentage *** sub_train_set*************   ',sub_train_x_y_values.shape[0]* 100/train_size )\n",
    "        #print('pourcentage **** sub_train patients *************   ',sub_train_patient* 100/train_patient )\n",
    "\n",
    "        #else:\n",
    "        #if(data=='val_set'):\n",
    "        val_train_x_y_values=load_pickle( DATA_DIR + '/from_raw/XY/val_set_x_y_raw.pickle')\n",
    "        val_set_size=val_train_x_y_values.shape[0]\n",
    "        val_patient=len(val_train_x_y_values['patient'].unique())\n",
    "        print('pourcentage *** val_set *************   ',val_train_x_y_values.shape[0]* 100/train_size )\n",
    "        print(\"\\n\")\n",
    "        print('pourcentage **** sub_train patients *************   ',sub_train_patient* 100/train_patient )\n",
    "        print('pourcentage **** val patients *************   ',val_patient* 100/train_patient )\n",
    "\n",
    "    print(\"--------------\")\n",
    "    print('number of all patients  '+data +'     ',len(x_y_values['patient'].unique()))\n",
    "    is_septic_list=0\n",
    "    non_septic_list=0\n",
    "    is_septic=0\n",
    "    non_septic=0\n",
    "    group=x_y_values.groupby(['patient'])\n",
    "    for i,g in group:\n",
    "        #df1= psv_to_dataframe(fname)\n",
    "        #list_df.append(df1)\n",
    "        if np.any(g['SepsisLabel']):\n",
    "            is_septic +=1 \n",
    "            is_septic_list += g.shape[0]\n",
    "        else:\n",
    "            non_septic +=1\n",
    "            non_septic_list += g.shape[0]\n",
    "    print ('number of positif patient  ',is_septic)\n",
    "    print ('number of negatif patient  ',non_septic)\n",
    "    print(\"\\n\")\n",
    "    print ('number of all observations for positif patients   ',is_septic_list)\n",
    "    positive_patient_list=x_y_values['patient'][x_y_values['SepsisLabel']==1.0].unique()\n",
    "    positive_patient_df=x_y_values[(x_y_values.patient.isin(positive_patient_list))]\n",
    "    \n",
    "    #print('number of positives observations/ number of negatives observations for positif patients  '+data +'  \\n',positive_patient_df['SepsisLabel'].value_counts())\n",
    "    print('where :::::::::  '+data +'  \\n',positive_patient_df['SepsisLabel'].value_counts())\n",
    "    print(\"\\n\")\n",
    "    print ('number of all observations for negatif paients   ',non_septic_list)\n",
    "    #list of negative  patients\n",
    "    negative_patient_list=x_y_values['patient'][x_y_values['SepsisLabel']==0.0].unique()\n",
    "    # construct dataframe of negative patients\n",
    "    negative_patient_df=x_y_values[x_y_values.patient.isin(negative_patient_list)]\n",
    "    #print('negative_patient_list  '+data +'     ',len(negative_patient_list))\n",
    "    \n",
    "    #list of positive patients\n",
    "    \n",
    "    # construct dataframe of positive patients\n",
    "    #positive_patient_df=x_y_values[(x_y_values.patient.isin(positive_patient_list))]\n",
    "    #positive_patient_df = x_y_values[~(x_y_values['patient'].isin(negative_patient_list))]\n",
    "    #print('positive_patient_list  '+data +'     ',len(positive_patient_list))\n",
    "    print(\"--------------\")\n",
    "    print('all observations  '+data +'     ',x_y_values.shape[0])\n",
    "    \n",
    "    #print('negatiffffffffffffffffffffff observations  '+data +'   ',negative_patient_df['SepsisLabel'].value_counts())\n",
    "    #print('negatives and positives observations  '+data +'  \\n',x_y_values['SepsisLabel'].value_counts())\n",
    "    print('where :::  '+data +'  \\n',x_y_values['SepsisLabel'].value_counts())\n",
    "    print(\"--------------\")\n",
    "    \n",
    "#mean=load_pickle(DATA_DIR + '/mean/mean_std.pickle')\n",
    "#display('mean  '+' display  x ',mean.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbd38157",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
